{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import Conv3d, LSTM, MaxPool3d, BatchNorm1d, BatchNorm3d, ZeroPad3d, Dropout, Linear, Flatten, Module, Sequential\n",
    "from torch.nn import functional as F\n",
    "\n",
    "class Conv3D_LSTM(Module):\n",
    "    def __init__(self, out_classes):\n",
    "        super().__init__()\n",
    "\n",
    "        # conv input: (None,3,20,128,128)\n",
    "        # lstm input: (None,20,132)\n",
    "\n",
    "        self.conv = Sequential(\n",
    "\n",
    "            Conv3d(in_channels=3,\n",
    "                   out_channels=16,\n",
    "                   kernel_size=(3,3,3),\n",
    "                   stride=(1,1,1),\n",
    "                   padding=(1,1,1)\n",
    "                   ),\n",
    "            MaxPool3d(kernel_size=(1,2,2),\n",
    "                      stride=(1,2,2)),\n",
    "            BatchNorm3d(16),\n",
    "\n",
    "            Conv3d(in_channels=16,\n",
    "                   out_channels=32,\n",
    "                   kernel_size=(3,3,3),\n",
    "                   stride=(1,1,1),\n",
    "                   padding=(1,1,1)\n",
    "                   ),\n",
    "            MaxPool3d(kernel_size=(2,2,2),\n",
    "                      stride=(2,2,2)),\n",
    "            BatchNorm3d(32),\n",
    "\n",
    "            Conv3d(in_channels=32,\n",
    "                   out_channels=64,\n",
    "                   kernel_size=(3,3,3),\n",
    "                   stride=(1,1,1),\n",
    "                   padding=(1,1,1)\n",
    "                   ),\n",
    "            MaxPool3d(kernel_size=(2,2,2),\n",
    "                      stride=(2,2,2)),\n",
    "            BatchNorm3d(64),\n",
    "\n",
    "            Conv3d(in_channels=64,\n",
    "                   out_channels=128,\n",
    "                   kernel_size=(3,3,3),\n",
    "                   stride=(1,1,1),\n",
    "                   padding=(1,1,1)\n",
    "                   ),\n",
    "            MaxPool3d(kernel_size=(1,2,2),\n",
    "                      stride=(1,2,2)),\n",
    "            BatchNorm3d(128),\n",
    "\n",
    "            ZeroPad3d((0,0,0,0,1,2)), \n",
    "            MaxPool3d(kernel_size=(2,2,2), # 128,4,4,4\n",
    "                      stride=(2,2,2)),\n",
    "            Flatten() # 2**13 = 8192 features\n",
    "        )\n",
    "        # input is a tensor of shape (sequence_length (L), input_size (Hin))\n",
    "        # LSTM(input_size = 10 (Hin), hidden_size = 20 (H_out), num_layers = 3)\n",
    "        # input = tensor(5,3,10)\n",
    "        # h0 = tensor(2,3,20)\n",
    "        # c0 = tensor(2,3,20)\n",
    "        # out =>\n",
    "\n",
    "        self.lstm1 = LSTM(input_size=132,hidden_size=66,num_layers=1)\n",
    "            # this require h_0 and c_0 of [1,20,66]\n",
    "            # return None,20,66\n",
    "        self.bn1 = BatchNorm1d(20)\n",
    "        self.drop = Dropout(.2)\n",
    "        self.lstm2 = LSTM(input_size=66,hidden_size=22,num_layers=1)\n",
    "            # return None, 20, 22\n",
    "        self.bn2 = BatchNorm1d(20)\n",
    "        self.flat = Flatten()\n",
    "        \n",
    "        self.final = Sequential(\n",
    "            Linear(in_features=8192,out_features=out_classes)\n",
    "            # Linear(in_features=100,out_features=out_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, frames, marks):\n",
    "        frames = frames.permute(0,4,1,2,3)\n",
    "\n",
    "        # h0 = torch.zeros((1,20,66)).cuda()\n",
    "        # c0 = torch.zeros((1,20,66)).cuda()\n",
    "\n",
    "        # h1 = torch.zeros((1,20,22)).cuda()\n",
    "        # c1 = torch.zeros((1,20,22)).cuda()\n",
    "\n",
    "        branch1 = self.conv(frames)\n",
    "\n",
    "        # branch2, (h0,c0) = self.lstm1(marks,(h0,c0))\n",
    "        # branch2 = self.bn1(branch2)\n",
    "        # branch2 = self.drop(branch2)\n",
    "\n",
    "        # branch2, (h1,c1) = self.lstm2(branch2,(h1,c1))\n",
    "        # branch2 = self.flat(branch2)\n",
    "        # del h0, c0, h1, c1\n",
    "        # out1 = torch.concat([branch1,branch2],dim=1)\n",
    "\n",
    "        out1 = F.relu_(branch1)\n",
    "        out1 = self.final(out1)\n",
    "        return F.softmax(out1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_16528\\337390636.py:102: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return F.softmax(out1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "Conv3D_LSTM                              [16, 2]                   60,800\n",
       "├─Sequential: 1-1                        [16, 8192]                --\n",
       "│    └─Conv3d: 2-1                       [16, 16, 20, 128, 128]    1,312\n",
       "│    └─MaxPool3d: 2-2                    [16, 16, 20, 64, 64]      --\n",
       "│    └─BatchNorm3d: 2-3                  [16, 16, 20, 64, 64]      32\n",
       "│    └─Conv3d: 2-4                       [16, 32, 20, 64, 64]      13,856\n",
       "│    └─MaxPool3d: 2-5                    [16, 32, 10, 32, 32]      --\n",
       "│    └─BatchNorm3d: 2-6                  [16, 32, 10, 32, 32]      64\n",
       "│    └─Conv3d: 2-7                       [16, 64, 10, 32, 32]      55,360\n",
       "│    └─MaxPool3d: 2-8                    [16, 64, 5, 16, 16]       --\n",
       "│    └─BatchNorm3d: 2-9                  [16, 64, 5, 16, 16]       128\n",
       "│    └─Conv3d: 2-10                      [16, 128, 5, 16, 16]      221,312\n",
       "│    └─MaxPool3d: 2-11                   [16, 128, 5, 8, 8]        --\n",
       "│    └─BatchNorm3d: 2-12                 [16, 128, 5, 8, 8]        256\n",
       "│    └─ZeroPad3d: 2-13                   [16, 128, 8, 8, 8]        --\n",
       "│    └─MaxPool3d: 2-14                   [16, 128, 4, 4, 4]        --\n",
       "│    └─Flatten: 2-15                     [16, 8192]                --\n",
       "├─Sequential: 1-2                        [16, 2]                   --\n",
       "│    └─Linear: 2-16                      [16, 2]                   16,386\n",
       "==========================================================================================\n",
       "Total params: 369,506\n",
       "Trainable params: 369,506\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (Units.GIGABYTES): 38.64\n",
       "==========================================================================================\n",
       "Input size (MB): 63.08\n",
       "Forward/backward pass size (MB): 1336.93\n",
       "Params size (MB): 1.23\n",
       "Estimated Total Size (MB): 1401.25\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchinfo import summary\n",
    "model = Conv3D_LSTM(2).to(device)\n",
    "input_frame = torch.randn(16,20,128,128,3).to(device)\n",
    "input_mark = torch.randn(16,20,132).to(device)\n",
    "output = model(input_frame,input_mark)\n",
    "summary(model,input_data=(input_frame,input_mark))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "import os\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "num_frames = 20 # changing requires model refactoring\n",
    "\n",
    "def load_video(directory):\n",
    "    cap = cv2.VideoCapture(directory)\n",
    "    frames = []\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if ret:\n",
    "            frames.append(cv2.cvtColor(frame,cv2.COLOR_BGR2RGB))\n",
    "        else:\n",
    "            break\n",
    "    cap.release()\n",
    "    return np.array(frames)/255\n",
    "\n",
    "def load_txt(directory):\n",
    "    df = pd.read_csv(directory)\n",
    "    dataset = df.iloc[1:,1:]\n",
    "    return np.array(dataset)\n",
    "\n",
    "def load_data(directory):\n",
    "    label_count = 0\n",
    "    X_train = []\n",
    "    y_train = []\n",
    "    list_label = os.listdir(directory)\n",
    "    for label in list_label:\n",
    "        count = 0\n",
    "        subpath = os.path.join(directory,label)\n",
    "        files = os.listdir(subpath)\n",
    "\n",
    "        files_txt = [txt for txt in files if txt.endswith(\".txt\")]\n",
    "        files_mp4 = [mp4 for mp4 in files if mp4.endswith(\".mp4\")]\n",
    "\n",
    "        if len(files_txt)!=len(files_mp4):\n",
    "            raise RuntimeError(\"The amount of .txt and .mp4 files are not equal. Found {} .mp4 but {} .txt\".format(len(files_mp4),len(files_txt)))\n",
    "        \n",
    "        n = len(files_txt)\n",
    "        \n",
    "        for i in range(n):\n",
    "            frames = load_video(os.path.join(subpath,files_mp4[i]))\n",
    "            marks = load_txt(os.path.join(subpath,files_txt[i]))\n",
    "            n_samples = marks.shape[0]\n",
    "            if count>=1000:\n",
    "                break\n",
    "            for j in range(num_frames, n_samples, num_frames):\n",
    "                count+=1\n",
    "                X_train.append( [frames[j-num_frames:j,::], marks[j-num_frames:j,:]] )\n",
    "                y_train.append(label_count)\n",
    "        label_count+=1\n",
    "    return X_train, y_train, list_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1216 1216\n",
      "['barbell biceps curl', 'bench press']\n"
     ]
    }
   ],
   "source": [
    "directory = \"Dual data processed/\"\n",
    "X_train, y_train, list_label = load_data(directory)\n",
    "print(len(X_train),len(y_train))\n",
    "print(list_label)\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_train, y_train, test_size=0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class SelfDataset(Dataset):\n",
    "    # self define a dataset\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self,index):\n",
    "        return self.X[index], self.y[index]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training preparation\n",
    "from torch import optim\n",
    "optimizer = optim.Adamax(model.parameters(),lr=0.001,weight_decay=1e-4)\n",
    "loss = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "batch_size = 16\n",
    "train_set = SelfDataset(X_train,y_train)\n",
    "train_loader = DataLoader(train_set, batch_size = batch_size, shuffle=True)\n",
    "test_set = SelfDataset(X_test,y_test)\n",
    "test_loader = DataLoader(test_set, batch_size = batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch_count = 0\n",
    "best_val_loss = 1000.\n",
    "save_checkpoint = \"test_model_convlstm\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 20, 128, 128, 3])\n",
      "torch.Size([16, 20, 132])\n"
     ]
    }
   ],
   "source": [
    "test_count = 0\n",
    "for input,label in train_loader:\n",
    "    test_count +=1\n",
    "    print(input[0].shape)\n",
    "    print(input[1].shape)\n",
    "    # print(input.shape)\n",
    "    if test_count == 1:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_16528\\4001747169.py:17: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  inp_frame = torch.tensor(input[0],dtype=torch.float32).to(device)\n",
      "C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_16528\\4001747169.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  inp_mark = torch.tensor(input[1],dtype=torch.float32).to(device)\n",
      "C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_16528\\337390636.py:102: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return F.softmax(out1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy: 0.7805212736129761, train loss: 23.688514709472656\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_16528\\4001747169.py:38: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  vinp_frame = torch.tensor(vinput[0],dtype=torch.float32).to(device)\n",
      "C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_16528\\4001747169.py:39: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  vinp_mark = torch.tensor(vinput[1],dtype=torch.float32).to(device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy: 0.8888888955116272, validation loss: 13.252412796020508\n",
      "Epoch 2\n",
      "Train accuracy: 0.9382715821266174, train loss: 17.55124855041504\n",
      "Validation accuracy: 0.8888888955116272, validation loss: 11.855677604675293\n",
      "Epoch 3\n",
      "Train accuracy: 0.957476019859314, train loss: 16.481412887573242\n",
      "Validation accuracy: 1.0, validation loss: 9.837172508239746\n",
      "Epoch 4\n",
      "Train accuracy: 0.9670782089233398, train loss: 16.112485885620117\n",
      "Validation accuracy: 1.0, validation loss: 10.404129981994629\n",
      "Epoch 5\n",
      "Train accuracy: 0.991769552230835, train loss: 14.974200248718262\n",
      "Validation accuracy: 1.0, validation loss: 9.787976264953613\n",
      "Epoch 6\n",
      "Train accuracy: 0.9986282587051392, train loss: 14.592756271362305\n",
      "Validation accuracy: 1.0, validation loss: 9.930070877075195\n",
      "Epoch 7\n",
      "Train accuracy: 0.9986282587051392, train loss: 14.542840957641602\n",
      "Validation accuracy: 1.0, validation loss: 9.726542472839355\n",
      "Epoch 8\n",
      "Train accuracy: 1.0, train loss: 14.50676155090332\n",
      "Validation accuracy: 1.0, validation loss: 9.717704772949219\n",
      "Epoch 9\n",
      "Train accuracy: 1.0, train loss: 14.444158554077148\n",
      "Validation accuracy: 1.0, validation loss: 9.71657943725586\n",
      "Epoch 10\n",
      "Train accuracy: 1.0, train loss: 14.43934440612793\n",
      "Validation accuracy: 1.0, validation loss: 9.711946487426758\n"
     ]
    }
   ],
   "source": [
    "# training loop:\n",
    "from torchmetrics import Accuracy\n",
    "\n",
    "acc = Accuracy(task=\"multiclass\",num_classes = 2).cuda()\n",
    "epoch_train = 10\n",
    "model.to(device)\n",
    "\n",
    "for epoch in range(epoch_train):\n",
    "    print(\"Epoch \" + str(epoch_count+1))\n",
    "    \n",
    "    model.train(True)\n",
    "    # train section\n",
    "    train_loss = 0.\n",
    "    for input, label in train_loader:\n",
    "\n",
    "        label = label.to(device)\n",
    "        inp_frame = torch.tensor(input[0],dtype=torch.float32).to(device)\n",
    "        inp_mark = torch.tensor(input[1],dtype=torch.float32).to(device)\n",
    "        optimizer.zero_grad()\n",
    "        # input (frames, mark)\n",
    "        output = model(inp_frame,inp_mark)\n",
    "        \n",
    "        los = loss(output,label)\n",
    "        los.backward()\n",
    "        optimizer.step()\n",
    "        acc.update(output,label)\n",
    "        train_loss += los\n",
    "\n",
    "    print(\"Train accuracy: {}, train loss: {}\".format(acc.compute(), train_loss))\n",
    "    \n",
    "    model.eval()\n",
    "    acc.reset()\n",
    "    val_loss = 0.\n",
    "    with torch.no_grad():\n",
    "        for vinput, vlabel in test_loader:\n",
    "            \n",
    "            vlabel = vlabel.to(device)\n",
    "            vinp_frame = torch.tensor(vinput[0],dtype=torch.float32).to(device)\n",
    "            vinp_mark = torch.tensor(vinput[1],dtype=torch.float32).to(device)\n",
    "            \n",
    "            voutput = model(vinp_frame,vinp_mark)\n",
    "            vloss = loss(output,label)\n",
    "            acc.update(output,label)\n",
    "            val_loss += vloss\n",
    "\n",
    "    print(\"Validation accuracy: {}, validation loss: {}\".format(acc.compute(), val_loss))\n",
    "    acc.reset()\n",
    "    if val_loss<best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        torch.save(model.state_dict(), save_checkpoint+\"_\"+str(epoch_count+1))\n",
    "    \n",
    "    epoch_count+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test no: 1\n",
      "tensor([0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_16528\\2134403992.py:6: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  input_fr = torch.tensor(input[0],dtype=torch.float32).to(device)\n",
      "C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_16528\\2134403992.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  input_mk = torch.tensor(input[1],dtype=torch.float32).to(device)\n",
      "C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_16528\\337390636.py:102: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return F.softmax(out1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 1 0 0 0 0 1 1 1 0 0 1 1 1 0]\n",
      "Test no: 2\n",
      "tensor([1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1])\n",
      "[1 1 1 0 0 1 1 1 0 1 0 0 0 0 1 1]\n",
      "Test no: 3\n",
      "tensor([0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0])\n",
      "[0 0 1 1 0 0 0 1 0 1 1 0 0 0 0 0]\n",
      "Test no: 4\n",
      "tensor([0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1])\n",
      "[0 0 0 1 0 0 0 0 1 0 1 1 1 0 1 1]\n",
      "Test no: 5\n",
      "tensor([0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0])\n",
      "[0 0 0 0 1 1 0 0 0 1 0 1 1 0 1 0]\n"
     ]
    }
   ],
   "source": [
    "# model.load_state_dict(torch.load(\"test_model_convlstm_9\"))\n",
    "count = 5\n",
    "for input, label in test_loader:\n",
    "    print(\"Test no: \" + str(6-count))\n",
    "    print(label)\n",
    "    input_fr = torch.tensor(input[0],dtype=torch.float32).to(device)\n",
    "    input_mk = torch.tensor(input[1],dtype=torch.float32).to(device)\n",
    "    output = model(input_fr,input_mk)\n",
    "    output = output.cpu().detach().numpy()\n",
    "    print(np.argmax(output,axis=1))\n",
    "    count-=1\n",
    "    if count == 0:\n",
    "        break\n",
    "     "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
